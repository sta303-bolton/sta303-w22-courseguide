# Bits and pieces

## Code to generate course art

```
# install.packages('readr')
# install.packages('tidyverse')
# install.packages("devtools")
# devtools::install_github("BlakeRMills/MetBrewer")

library(readr)
library(MetBrewer)
library(tidyverse)

course_code <- "STA303"

my_colours <- c(met.brewer("Cross", n = 8), met.brewer("Cross", n = 9))

set.seed(parse_number(course_code))

ngroup=17
names=paste("G_",seq(1,ngroup),sep="")
DAT=data.frame()

for(i in seq(1:30)){
  data=data.frame( matrix(0, ngroup , 3))
  data[,1]=i
  data[,2]=sample(names, nrow(data))
  data[,3]=prop.table(sample( c(rep(0,100),c(1:ngroup)) ,nrow(data)))
  DAT=rbind(DAT,data)
}
colnames(DAT)=c("Year","Group","Value")
DAT=DAT[order( DAT$Year, DAT$Group) , ]

ggplot(DAT, aes(x=Year, y=rev(Value), fill=Group )) +
  geom_area(alpha=1  )+
  theme_bw() +
  scale_fill_manual(values = my_colours)+
  theme(
    text = element_blank(),
    line = element_blank(),
    title = element_blank(),
    legend.position="none",
    panel.border = element_blank(),
    panel.background = element_blank(),
    plot.margin = margin(0, -2.7, 0, -2.7, "cm"))

ggsave(paste0(course_code, "-base.png"), width = 24, height = 2)

```

## M1 supporting information on matrices (not assessed) {#matrices}

### Background

#### Some true things about matrices
- The **rank** of a matrix is the number of linearly independent columns your matrix has.  
- If the number of columns = the rank of the matrix all the columns are **linearly independent**. If the umber of columns is > the rank of the matrix, all the columns are _not_ linearly independent.  
- You can only **invert** a square matrix if all its columns are linearly independent. (Determinant non-zero).


**Why do we care?**  In linear regression, to estimate $\boldsymbol\beta$, our vector of coefficients, we calculate $(\boldsymbol X^T\boldsymbol X)^{-1}\boldsymbol X^T\boldsymbol Y$. The elements of $\boldsymbol\beta$ can't be estimated if $\boldsymbol X^T\boldsymbol X$ (a square matrix) isn't invertible.

Clarification to what I said in the tests activity: We usually perform regression with an intercept because we don't want to assume out line passes through the origin, **0**. So, if there is an intercept, (column of 1s in the model matrix) we must convert every categorical variable with $k$ levels into $k-1$ dummy variables to have the intercept and still satisfies linear independence. IF we ditch the intercept, we can have k dummies, but this is only usually useful in the specific case of ANOVA. 


### Example 

```{r, message=FALSE}
library(tidyverse)
library(palmerpenguins)
```

```{r}
# function to replace NAs with 0 and text with 1
dummify <- function(x){
  if_else(is.na(x), 0, 1)
}

# create a smaller toy version of the penguin dataset (just for diplay purposes)
set.seed(24601)
pengwings <- penguins %>% 
  group_by(species) %>% 
  sample_n(4) %>% 
  select(body_mass_g, species)

pengwings

# creating a version of the data where there is a dummy column for each level of species instead of one species column
wider <- pengwings %>% 
  pivot_wider(id_cols = everything(), names_from = species, values_from = species, names_prefix = "species.") %>% 
  mutate_at(vars(starts_with("species.")), dummify)

wider
```

#### Classic regression, k-1 dummies (intercept)

```{r}
mod1 <- lm(body_mass_g ~ species, data = pengwings)
summary(mod1)
head(model.matrix(mod1))
```

Does the rank of the model matrix equal the number of columns?
```{r}
qr(model.matrix(mod1))$rank == ncol(model.matrix(mod1))
```

Okay, linearly independent, we're good to go!

#### Classic regression but trying to force k dummies (with an intercept)
```{r}
mod2 <- lm(body_mass_g ~ species.Adelie + species.Gentoo + species.Chinstrap, data=wider)
summary(mod2)
model.matrix(mod2)
qr(model.matrix(mod2))$rank
```

Does the rank of the model matrix equal the number of columns?
```{r}
qr(model.matrix(mod2))$rank == ncol(model.matrix(mod2))
```

Not linearly independent. Why?

Well, this intercept column is a linear combination of the three species columns!


```{r}
m <- model.matrix(mod2)

m[,1] == m[,2] + m[,3] + m[,4]
```
### Regression NOT classic, actually ANOVA! (no intercept)
```{r}
mod3 <- lm(body_mass_g ~ 0 + species.Adelie + species.Gentoo + species.Chinstrap, data=wider)
summary(mod3)
model.matrix(mod3)
qr(model.matrix(mod3))$rank
```

_Challenge question for +100 stats respect points: How do you interpret these coefficients?_

Does the rank of the model matrix equal the number of columns?
```{r}
qr(model.matrix(mod3))$rank == ncol(model.matrix(mod3))
```

Great, back to being linearly independent.

### Further reading (if you want it)

As I said at the beginning, I'm not planning to assess you on any of this. If you're interested in knowing more, or just think matrix algebra is delicious, I think this is a delightfully approachable walk through. https://online.stat.psu.edu/stat462/node/132/ 

## _p_ values (recap)

Remember: "Small but mighty!" The _smaller_ the _p_ value, the _stronger_ the evidence against the null hypothesis.

- We should never be making claims in in favour/against the alternative hypothesis. Our _statistical_ claims are always about the null. This may inform practical decisions in less strict ways, depending on practical significance, risks, etc., but a correct statistical claim for a _p_ value should ALWAYS be about the null. Absolutes are rare in teaching statistics.  
- A common threshold for rejecting or failing to reject the null hypothesis is 0.05. This is mostly from habit/convention, rather than some truly meaningful cosmic value. "Yeah, 1 in 20? Unlikely, sure." Difference disciplines/sub-disciplines develop norms appropriate to their context. 
- Many statisticians—especially in light of the reproducibility crisis and poor public, and even sometimes researcher, understanding of _p_ values—prefer to make statements about the __strength__ of evidence, not just reject/fail to reject.

+--------------------------+----------------------------------------------------+
| _p_ value range            | Strength comment                                   |
+==========================+====================================================+
| \> 0.1                   | No evidence against the null                       |
+--------------------------+----------------------------------------------------+
| 0.05 \< _p_ value \< 0.1   | Weak evidence against the null hypothesis          |
+--------------------------+----------------------------------------------------+
| 0.01 \< _p_ value \< 0.05   | Moderate/some evidence against the null hypothesis |
+--------------------------+----------------------------------------------------+
| 0.001 \< _p_ value \< 0.01 | Strong evidence against the null hypothesis        |
+--------------------------+----------------------------------------------------+
| \< 0.001                 | Very strong evidence against the null hypothesis   |
+--------------------------+----------------------------------------------------+

### What if you get a value that is exactly one of these thresholds?

It would be very rare to get a _p_ value of exactly one of these thresholds. If you were to, you could decide based on your judgment of the context. Should you be more or less conservative in your interpretation based on the needs of the project/client?

So, you'll almost always have something, that while it may round to 0.001 (or another threshold), is really something like 0.001345789 or 0.00098963. What judgment would you make in each case, with this in mind?

But remember, these are __fuzzy boundaries based on easy-to-grasp probabilities for humans__, particularly humans used to thinking in base 10. 1 in 20, 1 in 100, 1 in 1000, etc. Easy to think about but not universally meaningful thresholds. Honestly, being a little over or under really doesn't make that much of a difference. _p_ values are continuous random variables. We impose discrete categories for our own ease. Used too rigidly, these guidelines have the same problem as rejecting/failing to reject, but just with a few more levels.

