# (PART\*) Appendix {-}

# Bits and pieces

## Code to generate course art

```
# install.packages('readr')
# install.packages('tidyverse')
# install.packages("devtools")
# devtools::install_github("BlakeRMills/MetBrewer")

library(readr)
library(MetBrewer)
library(tidyverse)

course_code <- "STA303"

my_colours <- c(met.brewer("Cross", n = 8), met.brewer("Cross", n = 9))

set.seed(parse_number(course_code))

ngroup=17
names=paste("G_",seq(1,ngroup),sep="")
DAT=data.frame()

for(i in seq(1:30)){
  data=data.frame( matrix(0, ngroup , 3))
  data[,1]=i
  data[,2]=sample(names, nrow(data))
  data[,3]=prop.table(sample( c(rep(0,100),c(1:ngroup)) ,nrow(data)))
  DAT=rbind(DAT,data)
}
colnames(DAT)=c("Year","Group","Value")
DAT=DAT[order( DAT$Year, DAT$Group) , ]

ggplot(DAT, aes(x=Year, y=rev(Value), fill=Group )) +
  geom_area(alpha=1  )+
  theme_bw() +
  scale_fill_manual(values = my_colours)+
  theme(
    text = element_blank(),
    line = element_blank(),
    title = element_blank(),
    legend.position="none",
    panel.border = element_blank(),
    panel.background = element_blank(),
    plot.margin = margin(0, -2.7, 0, -2.7, "cm"))

ggsave(paste0(course_code, "-base.png"), width = 24, height = 2)

```

## M1 supporting information on matrices (not assessed) {#matrices}

### Background

#### Some true things about matrices
- The **rank** of a matrix is the number of linearly independent columns your matrix has.  
- If the number of columns = the rank of the matrix all the columns are **linearly independent**. If the umber of columns is > the rank of the matrix, all the columns are _not_ linearly independent.  
- You can only **invert** a square matrix if all its columns are linearly independent. (Determinant non-zero).


**Why do we care?**  In linear regression, to estimate $\boldsymbol\beta$, our vector of coefficients, we calculate $(\boldsymbol X^T\boldsymbol X)^{-1}\boldsymbol X^T\boldsymbol Y$. The elements of $\boldsymbol\beta$ can't be estimated if $\boldsymbol X^T\boldsymbol X$ (a square matrix) isn't invertible.

Clarification to what I said in the tests activity: We usually perform regression with an intercept because we don't want to assume out line passes through the origin, **0**. So, if there is an intercept, (column of 1s in the model matrix) we must convert every categorical variable with $k$ levels into $k-1$ dummy variables to have the intercept and still satisfies linear independence. IF we ditch the intercept, we can have k dummies, but this is only usually useful in the specific case of ANOVA. 


### Example 

```{r, message=FALSE}
library(tidyverse)
library(palmerpenguins)
```

```{r}
# function to replace NAs with 0 and text with 1
dummify <- function(x){
  if_else(is.na(x), 0, 1)
}

# create a smaller toy version of the penguin dataset (just for diplay purposes)
set.seed(24601)
pengwings <- penguins %>% 
  group_by(species) %>% 
  sample_n(4) %>% 
  select(body_mass_g, species)

pengwings

# creating a version of the data where there is a dummy column for each level of species instead of one species column
wider <- pengwings %>% 
  pivot_wider(id_cols = everything(), names_from = species, values_from = species, names_prefix = "species.") %>% 
  mutate_at(vars(starts_with("species.")), dummify)

wider
```

#### Classic regression, k-1 dummies (intercept)

```{r}
mod1 <- lm(body_mass_g ~ species, data = pengwings)
summary(mod1)
head(model.matrix(mod1))
```

Does the rank of the model matrix equal the number of columns?
```{r}
qr(model.matrix(mod1))$rank == ncol(model.matrix(mod1))
```

Okay, linearly independent, we're good to go!

#### Classic regression but trying to force k dummies (with an intercept)
```{r}
mod2 <- lm(body_mass_g ~ species.Adelie + species.Gentoo + species.Chinstrap, data=wider)
summary(mod2)
model.matrix(mod2)
qr(model.matrix(mod2))$rank
```

Does the rank of the model matrix equal the number of columns?
```{r}
qr(model.matrix(mod2))$rank == ncol(model.matrix(mod2))
```

Not linearly independent. Why?

Well, this intercept column is a linear combination of the three species columns!


```{r}
m <- model.matrix(mod2)

m[,1] == m[,2] + m[,3] + m[,4]
```
### Regression NOT classic, actually ANOVA! (no intercept)
```{r}
mod3 <- lm(body_mass_g ~ 0 + species.Adelie + species.Gentoo + species.Chinstrap, data=wider)
summary(mod3)
model.matrix(mod3)
qr(model.matrix(mod3))$rank
```

_Challenge question for +100 stats respect points: How do you interpret these coefficients?_

Does the rank of the model matrix equal the number of columns?
```{r}
qr(model.matrix(mod3))$rank == ncol(model.matrix(mod3))
```

Great, back to being linearly independent.

### Further reading (if you want it)

As I said at the beginning, I'm not planning to assess you on any of this. If you're interested in knowing more, or just think matrix algebra is delicious, I think this is a delightfully approachable walk through. https://online.stat.psu.edu/stat462/node/132/ 
